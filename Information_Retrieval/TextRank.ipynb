{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from pymystem3 import Mystem\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from bs4 import BeautifulSoup\n",
    "from inscriptis import get_text\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook\n",
    "from six import iteritems\n",
    "from nltk.corpus import stopwords \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = './data/data'\n",
    "path_index = './data/data_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def speller(q):\n",
    "    pos = 0\n",
    "    q_pos = []\n",
    "    q_split = q.split()\n",
    "    for i in q_split:\n",
    "        if pos > 0: pos += 1\n",
    "        q_pos.append(pos)\n",
    "        pos += len(i)\n",
    "    \n",
    "    url = 'https://speller.yandex.net/services/spellservice.json/checkText?text=' + q\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    if len(q_split) > 1 and len(response) == 1 and response[0]['len'] == len(q):\n",
    "        return q\n",
    "    \n",
    "    if len(response) > 0:\n",
    "        for spl in response:\n",
    "            if spl['pos'] in q_pos:\n",
    "                q_split[q_pos.index(spl['pos'])] = spl['s'][0]\n",
    "\n",
    "        return ' '.join(q_split)\n",
    "    else:\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Synonyms:\n",
    "    def __init__(self):\n",
    "        self.cache = dict()\n",
    "        \n",
    "    def get_syn(self, word):\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        \n",
    "        key = 'dict.1.1.20190324T123533Z.62c2a8f7b72801a9.3260c2a87968032222ed684baa0b0e9679ef58f0'\n",
    "        url = 'https://dictionary.yandex.net/api/v1/dicservice.json/lookup?lang=ru-ru&text=' + word + '&key=' + key\n",
    "        response = requests.get(url).json()\n",
    "                \n",
    "        result = []\n",
    "        if len(response) > 0 and 'def' in response and len(response['def']) > 0:\n",
    "            tr = response['def'][0]['tr']\n",
    "            tr_len = len(tr)\n",
    "            i = 0\n",
    "            path = tr[i]\n",
    "            \n",
    "            if 'text' in path:\n",
    "                result.append(path['text'])\n",
    "                \n",
    "            while tr_len > 0 and 'syn' not in path:\n",
    "                path = tr[i]\n",
    "                i += 1\n",
    "                tr_len -= 1\n",
    "                \n",
    "            if 'syn' in path:\n",
    "                result = result + [w['text'] for w in path['syn']]\n",
    "                result = [w for w in result if len(w.split(' ')) == 1][:5]\n",
    "                \n",
    "        self.cache[word] = result \n",
    "        return result\n",
    "        \n",
    "# synonyms = Synonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join('./_synonyms.json'), 'w') as idf:\n",
    "#     idf.write(json.dumps(synonyms.cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.cache = dict()\n",
    "        self.morph = Mystem()\n",
    "\n",
    "    def lemmatize(self, word):\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "\n",
    "        result = self.morph.lemmatize(word)[0]\n",
    "        self.cache[word] = result\n",
    "        return result\n",
    "\n",
    "get_lemma = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_num = re.compile('([\\d])[\\s]+([\\d])')\n",
    "regex_punct = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}—~–«»-•©№…'))\n",
    "stopwords_list = stopwords.words('russian') + \\\n",
    "                 stopwords.words('english') + \\\n",
    "                 ['а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у',\n",
    "                  'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "                  'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "def normalizer(text):\n",
    "    text = regex_punct.sub(' ', text)\n",
    "    text = regex_num.sub('\\\\1\\\\2', text)\n",
    "    text = text.lower()\n",
    "    terms = []\n",
    "    for word in text.split():\n",
    "        word = get_lemma.lemmatize(word)\n",
    "        if word not in stopwords_list:\n",
    "            terms.append(word)        \n",
    "\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = { \n",
    "    'title': 5, \n",
    "    'h1': 4, \n",
    "    'h2': 2.5,\n",
    "    'h3': 2,\n",
    "    'h4': 1.5,\n",
    "    'b' : 1.5, \n",
    "    'strong': 1.5, \n",
    "    'text': 1\n",
    "}\n",
    "\n",
    "def parse(path_file):\n",
    "    zones = {}\n",
    "        \n",
    "    with open(path_file) as opened_file:        \n",
    "        soup = BeautifulSoup(opened_file, 'html.parser')\n",
    "    \n",
    "    try:    \n",
    "        for zone in weights.keys():\n",
    "            zones[zone] = []\n",
    "            for item in soup.findAll(zone):\n",
    "                if item.string:\n",
    "                    zones[zone].extend(normalizer(item.string))\n",
    "\n",
    "        zones['text'] = normalizer(get_text(soup.prettify()))\n",
    "        \n",
    "    except:\n",
    "        try: \n",
    "            zones['text'] = normalizer(get_text(soup.prettify()))\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url = './data/data/doc.06052.dat'\n",
    "# extract = parse(url)\n",
    "# extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequency(extract):\n",
    "    terms = {}\n",
    "    terms_sum = {}\n",
    "    \n",
    "    for zone in extract.keys():\n",
    "        terms[zone] = {}\n",
    "        for word in extract[zone]:\n",
    "            # частота терма в каждой зоне\n",
    "            terms[zone][word] = terms[zone].get(word, 0) + 1\n",
    "\n",
    "    for zone in terms.keys():\n",
    "        for word in terms[zone].keys():\n",
    "            # умножаем частоту в каждой зоне на вес\n",
    "            terms[zone][word] = terms[zone][word] * weights[zone]\n",
    "    \n",
    "    for zone in terms.keys():\n",
    "        for word in terms[zone].keys():\n",
    "            # Складываем получившиеся частоты из разных зон\n",
    "            terms_sum[word] = terms_sum.get(word, 0) + terms[zone][word]\n",
    "    \n",
    "    return terms_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spl_queries():\n",
    "    with open('./data/queries.numerate.txt') as item:\n",
    "        with open('./data/queries.numerate.spl.txt', 'w') as spl:\n",
    "            for line in item:\n",
    "                q_id, q = line.strip().split('\\t')\n",
    "                try:\n",
    "                    spl.write(q_id + '\\t' + speller(q) + '\\n')\n",
    "                except:\n",
    "                    spl.write(q_id + '\\t' + q + '\\n')\n",
    "\n",
    "# spl_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(text):\n",
    "    return text.readline().strip()\n",
    "\n",
    "hashUrl_docId = {}        \n",
    "docId_urls = {}\n",
    "with open('./data/urls.numerate.txt') as item:\n",
    "    for line in item:\n",
    "        doc_id, url = line.strip().split('\\t')\n",
    "        docId_urls[int(doc_id)] = url\n",
    "        hashUrl_docId[hash(url)] = int(doc_id)\n",
    "        \n",
    "query_id = {}\n",
    "with open('./data/queries.numerate.spl.txt') as item:\n",
    "    for line in item:\n",
    "        q_id, q = line.strip().split('\\t')\n",
    "        query_id[q] = int(q_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCorpus():\n",
    "    index_files = os.listdir(path_index)\n",
    "    dictionary = Dictionary([])\n",
    "    corpus = []\n",
    "        \n",
    "    for item in tqdm_notebook(index_files, total=len(index_files), mininterval=1):\n",
    "        path_file = os.path.join(path_index, item)\n",
    "\n",
    "        try:\n",
    "            with open(path_file) as opened_file:\n",
    "                doc = DocIndex().load(path_file) \n",
    "                dictionary.add_documents([doc.terms])\n",
    "                corpus.append(dictionary.doc2bow(doc.terms))\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    with open(os.path.join(\"./_corpus.txt\"), 'w') as ifile:\n",
    "        ifile.write(json.dumps(corpus))\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "def LDA_model(corpus):\n",
    "    np.random.seed(17)\n",
    "    LDA = LdaModel(corpus, num_topics=10)\n",
    "    lda.save(\"./LDAmodel/model\")\n",
    "    \n",
    "    return LDA\n",
    "\n",
    "# corpus = getCorpus()\n",
    "# corpus = json.loads(open(\"./_corpus.txt\").read())\n",
    "# LDA = LDA_model(corpus)\n",
    "# LDA = LdaModel.load(\"./LDAmodel/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./data/sample_submission.txt')\n",
    "\n",
    "class Query():     \n",
    "    def __init__(self, query):\n",
    "        if not query in query_id: \n",
    "            return\n",
    "\n",
    "        self.id = query_id[query]\n",
    "        self.docs_id = sample_submission[sample_submission['QueryId'] == self.id]['DocumentId'].values        \n",
    "        self.terms = normalizer(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocIndex():    \n",
    "    def __init__(self):\n",
    "        self.id = 0\n",
    "        self.url = ''\n",
    "        self.error = False\n",
    "        self.doc_len = 0\n",
    "        self.doc_freqs = {}\n",
    "        self.positions = {}\n",
    "        self.terms = []\n",
    "        \n",
    "    def build_data(self, path_file, hashUrl):\n",
    "        self.id = hashUrl_docId[hashUrl]\n",
    "        self.url = docId_urls[self.id]\n",
    "\n",
    "        extract = parse(path_file)\n",
    "            \n",
    "        if not extract:\n",
    "            self.error = True\n",
    "        else:\n",
    "            # Все термы документа\n",
    "            self.terms = extract['text']\n",
    "            \n",
    "            # Позиция терм в документе\n",
    "            for i, term in enumerate(extract['text']):\n",
    "                if term not in self.positions:\n",
    "                    self.positions[term] = []\n",
    "                self.positions[term].append(i+1)\n",
    "            \n",
    "            # Длина документа\n",
    "            self.doc_len = len(extract['text'])\n",
    "            \n",
    "            # Объединяем частоту из разных зон\n",
    "            self.doc_freqs = get_frequency(extract)\n",
    "        return self\n",
    "    \n",
    "    def save(self, path_index):\n",
    "        with open(os.path.join(path_index, str(self.id)), 'w') as doc:\n",
    "            doc.write(json.dumps(self.__dict__))\n",
    "            \n",
    "    def load(self, path_index):\n",
    "        try:\n",
    "            params = json.loads(open(path_index).read())\n",
    "        except:\n",
    "            self.error = True\n",
    "            return self\n",
    "        \n",
    "        self.id = params['id']\n",
    "        self.url = params['url']\n",
    "        self.error = params['error']\n",
    "        self.doc_len = params['doc_len']\n",
    "        self.doc_freqs = params['doc_freqs']\n",
    "        self.positions = params['positions']\n",
    "        self.terms = params['terms']\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce2de833ce46449461bba849722893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Загружено: 0; Сохранено: 74223; err_no_url: 17; error: 18;\n"
     ]
    }
   ],
   "source": [
    "class IndexBuild(): \n",
    "    def __init__(self):\n",
    "        self.corpus_size = 0\n",
    "        self.average_idf = 0\n",
    "        self.avgdl = 0\n",
    "        self.docs_len = 0\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        total_load = 0\n",
    "        total_save = 0\n",
    "        self.err_no_url = []\n",
    "        self.errors = []\n",
    "        \n",
    "        index_files = os.listdir(path_index)\n",
    "        data_files = os.listdir(path_data)\n",
    "        \n",
    "        for item in tqdm_notebook(data_files, total=len(data_files), mininterval=1):\n",
    "            path_file = os.path.join(path_data, item)\n",
    "\n",
    "            try:\n",
    "                with open(path_file) as opened_file:\n",
    "                    hashUrl = hash(get_url(opened_file))\n",
    "                    if hashUrl not in hashUrl_docId:\n",
    "                        self.err_no_url.append(path_file)\n",
    "                        continue\n",
    "                    else:\n",
    "                        if str(hashUrl_docId[hashUrl]) in index_files:\n",
    "                            total_load += 1\n",
    "                            doc_index = DocIndex().load(os.path.join(path_index, str(hashUrl_docId[hashUrl]))) \n",
    "                        else:\n",
    "                            doc_index = DocIndex().build_data(path_file, hashUrl)\n",
    "                            if doc_index.error:\n",
    "                                self.errors.append((path_file))\n",
    "                                continue\n",
    "                            else:\n",
    "                                total_save += 1\n",
    "                                doc_index.save(path_index)\n",
    "\n",
    "                self.docs_len += doc_index.doc_len\n",
    "                self.corpus_size += 1\n",
    "                for word in doc_index.doc_freqs.keys():\n",
    "                    self.df[word] = self.df.get(word, 0) + 1\n",
    "\n",
    "            except:\n",
    "                self.errors.append((path_file))\n",
    "                continue\n",
    "\n",
    "        self.avgdl = self.docs_len / self.corpus_size\n",
    "        idf_sum = 0\n",
    "        eps = 0.25\n",
    "        negative_idfs = []\n",
    "        for word, freq in iteritems(self.df):\n",
    "            idf = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = idf_sum / len(self.idf)\n",
    "\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps * self.average_idf\n",
    "        \n",
    "        with open(os.path.join('./_params.json'), 'w') as ifile:\n",
    "            params = {\n",
    "                \"corpus_size\": self.corpus_size,\n",
    "                \"docs_len\": self.docs_len, \n",
    "                \"avgdl\": self.avgdl,\n",
    "                \"average_idf\": self.average_idf\n",
    "            }\n",
    "            ifile.write(json.dumps(params))\n",
    "        with open(os.path.join('./_idf.json'), 'w') as idf:\n",
    "            idf.write(json.dumps(self.idf))\n",
    "        \n",
    "        print(\"Загружено: {0}; Сохранено: {1}; err_no_url: {2}; error: {3};\".format(total_load, total_save, len(self.err_no_url), len(self.errors)))\n",
    "        return self\n",
    "        \n",
    "index = IndexBuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = json.loads(open(os.path.join('./_idf.json')).read())\n",
    "params = json.loads(open(os.path.join('./_params.json')).read())\n",
    "synonyms = json.loads(open(os.path.join('./_synonyms.json')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrase_score(phrase_words, phrase_pos, doc_id):    \n",
    "    score_phrase = 0\n",
    "    freq_phrase = 0\n",
    "    match_words = []\n",
    "    for i, pos in enumerate(phrase_pos[0]):\n",
    "        match_words.append(pos)\n",
    "        j = 1\n",
    "        stop = False\n",
    "        while j < len(phrase_words) and not stop:\n",
    "            check_pos = [pos+1]\n",
    "            for k, item in enumerate(check_pos): \n",
    "                if item in phrase_pos[j]:\n",
    "                    match_words.append(item)\n",
    "                    pos = item\n",
    "                    break\n",
    "                else:\n",
    "                    if k == len(check_pos)-1:\n",
    "                        stop = True\n",
    "                        break\n",
    "            j += 1\n",
    "\n",
    "        if len(match_words) == len(phrase_words):\n",
    "            freq_phrase += 1\n",
    "        match_words = []\n",
    "\n",
    "    if freq_phrase > 0:\n",
    "        score_phrase = 0.1 * sum([idf[w] for w in phrase_words]) * (freq_phrase/(1+freq_phrase))\n",
    "    \n",
    "    return score_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranging(query):\n",
    "    result = []\n",
    "    k = 2\n",
    "    b = 0.75\n",
    "    \n",
    "    q = Query(query)\n",
    "    \n",
    "    for doc_id in q.docs_id:\n",
    "        doc_index = DocIndex().load(os.path.join(path_index, str(doc_id))) \n",
    "        if doc_index.error: \n",
    "            continue\n",
    "            \n",
    "        doc_freqs = doc_index.doc_freqs\n",
    "        dl = doc_index.doc_len\n",
    "        avgdl = params['avgdl']\n",
    "        score = 0\n",
    "        \n",
    "        # BM25F\n",
    "        score_bm25 = 0\n",
    "        for word in q.terms:\n",
    "            if word in doc_freqs:      \n",
    "                f = doc_freqs[word]\n",
    "                TF = (f * (k + 1)) / (f + k * (1 - b + b * (dl / avgdl)))\n",
    "                score_bm25 += idf[word] * TF\n",
    "            else:\n",
    "                # Учет синонимов для отсутствующих слов из запроса\n",
    "                # get_synonym = synonyms.get_syn(word)\n",
    "                get_synonym = synonyms[word][:1]\n",
    "                for w_syn in get_synonym:\n",
    "                    if w_syn in doc_freqs:\n",
    "                        f = doc_freqs[w_syn]\n",
    "                        score_bm25 += 0.1 * idf[w_syn] * (f/(1+f))\n",
    "                        break\n",
    "\n",
    "                \n",
    "        # Pair - Сколько раз было точное вхождение пары слов из запроса\n",
    "        positions = doc_index.positions\n",
    "        pair = []\n",
    "        phrase_pos = []\n",
    "        phrase_words = []\n",
    "        for i, word in enumerate(q.terms):\n",
    "            if str(word) in positions:\n",
    "                phrase_pos.append(positions[str(word)])\n",
    "                phrase_words.append(str(word))     \n",
    "            if i+1 == len(q.terms): break\n",
    "            if str(q.terms[i]) in positions and str(q.terms[i+1]) in positions:\n",
    "                pair.append(([positions[str(q.terms[i])], positions[str(q.terms[i+1])]], [q.terms[i], q.terms[i+1]]))\n",
    "                \n",
    "        friq_pair = []\n",
    "        score_pair = 0\n",
    "        for i in range(len(pair)):\n",
    "            pos_pair = pair[i][0] \n",
    "            words = pair[i][1]\n",
    "            count = 0\n",
    "            for p in pos_pair[0]:\n",
    "                if p+1 in pos_pair[1]:\n",
    "                    count += 1\n",
    "            friq_pair.append(count)\n",
    "            score_pair += 0.1 * (idf[words[0]] + idf[words[1]]) * (count/(1+count))\n",
    "            \n",
    "            \n",
    "        # Phrase - Подряд все слова фразы, если она больше 2 слов\n",
    "        score_phrase = 0\n",
    "        if len(phrase_words) == len(q.terms) and len(phrase_words) > 2:\n",
    "            score_phrase = get_phrase_score(phrase_words, phrase_pos, doc_id)\n",
    "        \n",
    "        \n",
    "        score = score_bm25 + score_pair + score_phrase\n",
    "        result.append([score, q.id, doc_id])\n",
    "                \n",
    "    return sorted(result, key=itemgetter(0), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "with open('./data/queries.numerate.spl.txt') as item:\n",
    "    for line in item:\n",
    "        q_id, q = line.strip().split('\\t')\n",
    "        answers.extend(ranging(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(answers, columns=['Score', 'QueryId', 'DocumentId'])\n",
    "df[['QueryId', 'DocumentId']].to_csv('./predict.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id_doc - file_name\n",
    "docId_fileName = {}\n",
    "\n",
    "data_files = os.listdir(path_data)\n",
    "for item in data_files:\n",
    "    path_file = os.path.join(path_data, item)\n",
    "\n",
    "    with open(path_file) as opened_file:\n",
    "        hashUrl = hash(get_url(opened_file))\n",
    "        if hashUrl not in hashUrl_docId:\n",
    "            continue\n",
    "        else:\n",
    "            docId_fileName[hashUrl_docId[hashUrl]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9840, 9814, 9808, 9822, 9821, 9805, 9841, 9838, 9809, 9827, 9818,\n",
       "       9828, 9825, 9802, 9806, 9813, 9835, 9800, 9810, 9826, 9843, 9829,\n",
       "       9834, 9807, 9816, 9815, 9803, 9830, 9804, 9811, 9819, 9832, 9833,\n",
       "       9839, 9842, 9801, 9820, 9799, 9836, 9837, 9824, 9812, 9823, 9817,\n",
       "       9831])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('./data/sample_submission.txt')\n",
    "q_id = query_id['беспроводная зарядка для iphone xr']\n",
    "docs_id = sample[sample['QueryId'] == q_id]['DocumentId'].values \n",
    "docs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([docId_fileName[i] for i in docs_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
